{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d326a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "class Param:\n",
    "    def __init__(self, val = 0, grad = 0):\n",
    "        self.val = random.uniform(-5, 5)\n",
    "        self.grad = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a11964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.val = 0\n",
    "        self.grad = 0\n",
    "        self.w_params = [Param() for i in range(nin)]\n",
    "        self.b_param = Param()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.val}\"\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return self.val * other\n",
    "    def __rmul__(self, other):\n",
    "        return self.val * other \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e149336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, size):\n",
    "        self.neurons = [Neuron(nin) for i in range(size)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.neurons}\"\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85309225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y):\n",
    "    if(isinstance(y_pred, Layer)):\n",
    "        return sum([(y_pred.neurons[i].val - y[i])**2 for i in range(len(y_pred.neurons))])\n",
    "    \n",
    "def dot(v1, v2):\n",
    "    if(isinstance(v1, Layer)):\n",
    "        return sum([v1.neurons[i].val*v2[i].val for i in range(len(v1.neurons))])\n",
    "    else:\n",
    "        return sum([v1[i]*v2[i].val for i in range(len(v1))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1./(1 + math.exp(-x))\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, sizes):\n",
    "        self.layers = [Layer(sizes[i], sizes[i+1]) for i in range(len(sizes) - 1)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        for layer in self.layers:\n",
    "            layer.prev_layer = x\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.val = sigmoid(dot(x , neuron.w_params) + neuron.b_param.val)\n",
    "            x = layer\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def backward(self, y, batch_size=1):\n",
    "        #print(f\"loss is {loss(self.layers[-1], y)}\")\n",
    "        \n",
    "        # calc output grads & weight \n",
    "        for (neuron, y_val) in zip(self.layers[-1], y):\n",
    "            neuron.grad += 2 * (neuron.val - y_val) * (1/batch_size) # implement mse loss \n",
    "            for w_param, x_val in zip(neuron.w_params, self.layers[-1].prev_layer):\n",
    "                w_param.grad += (neuron.grad *\n",
    "                               (neuron.val**2) * \n",
    "                                math.exp(-(dot(self.layers[-1].prev_layer, neuron.w_params) + neuron.b_param.val)) *\n",
    "                                x_val)\n",
    "            neuron.b_param.grad += (neuron.grad *\n",
    "                               (neuron.val**2) * \n",
    "                                math.exp(-(dot(self.layers[-1].prev_layer, neuron.w_params) + neuron.b_param.val)))\n",
    "        # calc rest of grads\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            # calc neuron grads\n",
    "            for idx, neuron in enumerate(self.layers[i]):\n",
    "                # calc neuron grads\n",
    "                for next_neuron in self.layers[i + 1]:\n",
    "                    neuron.grad +=  (next_neuron.grad * \n",
    "                                     next_neuron.val**2 * \n",
    "                                     math.exp(-(dot(self.layers[i], next_neuron.w_params) + next_neuron.b_param.val))*\n",
    "                                     next_neuron.w_params[idx].val\n",
    "                                    )\n",
    "                # calc param grads \n",
    "                for w_param, x_val in zip(neuron.w_params, self.layers[i].prev_layer):\n",
    "                    w_param.grad += (neuron.grad *\n",
    "                                   (neuron.val**2) * \n",
    "                                    math.exp(-(dot(self.layers[i].prev_layer, neuron.w_params) + neuron.b_param.val)) *\n",
    "                                    x_val)\n",
    "                neuron.b_param.grad += (neuron.grad *\n",
    "                                   (neuron.val ** 2) *  \n",
    "                                   math.exp(-(dot(self.layers[i].prev_layer, neuron.w_params) + neuron.b_param.val))\n",
    "                                    )\n",
    "    def zero_neurons_grads(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.grad = 0\n",
    "    def zero_params_grads(self):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                #neuron.grad = 0\n",
    "                for w_param in neuron.w_params:\n",
    "                    w_param.grad = 0\n",
    "                neuron.b_param.grad = 0\n",
    "    \n",
    "    def step(self, lr=1):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                for w_param in neuron.w_params:\n",
    "                    w_param.val -= w_param.grad * lr\n",
    "                neuron.b_param.val -= neuron.b_param.val * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c68a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = [(0, 0), (0, 1), (1,0), (1, 1)]\n",
    "# y_data = [(0, ), (1, ), (1, ), (0, )]\n",
    "def train_MLP(mlp, x_data, y_data, num_iters = 1, lr=1):\n",
    "    # calc grads\n",
    "    for i in range(num_iters):\n",
    "        total_loss = 0; \n",
    "        mlp.zero_params_grads()\n",
    "        for sample in zip(x_data, y_data):\n",
    "            mlp.zero_neurons_grads()\n",
    "            ypred = mlp.forward(sample[0])\n",
    "            total_loss += loss(ypred, sample[1])\n",
    "            mlp.backward(sample[1], batch_size = len(x_data))\n",
    "        \n",
    "        if(i%100 == 0):\n",
    "            print(f\"Iteration {i}: Loss = {total_loss/len(x_data)}\")\n",
    "\n",
    "        # step \n",
    "        mlp.step()\n",
    "        \n",
    "    total_loss = 0; \n",
    "    #mlp.zero_params_grads()\n",
    "    for sample in zip(x_data, y_data):\n",
    "        mlp.zero_neurons_grads()\n",
    "        ypred = mlp.forward(sample[0])\n",
    "        total_loss += loss(ypred, sample[1])\n",
    "        #mlp.backward(sample[1], batch_size = len(x_data))\n",
    "            \n",
    "    print(f\"Final Loss = {total_loss/len(x_data)}\")\n",
    "\n",
    "def calc_loss(mlp, x_data, y_data):\n",
    "    total_loss = 0\n",
    "    for sample in zip(x_data, y_data):\n",
    "            ypred = mlp.forward(sample[0])\n",
    "            total_loss += loss(ypred, sample[1])\n",
    "    \n",
    "     print(f\"Loss on test set: {total_loss/len(x_data)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed06e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.4967955422007997\n",
      "Iteration 100: Loss = 0.04267504478950692\n",
      "Iteration 200: Loss = 0.01256912775458454\n",
      "Iteration 300: Loss = 0.007115362622937966\n",
      "Iteration 400: Loss = 0.00488596913241005\n",
      "Final Loss = 0.003691606765115635\n"
     ]
    }
   ],
   "source": [
    "# my_MLP = MLP([2, 10, 2, 1])\n",
    "\n",
    "# train_MLP(my_MLP, x_data, y_data, num_iters = 500, lr = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
