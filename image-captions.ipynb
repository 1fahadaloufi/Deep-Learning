{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-06T21:06:11.444845Z","iopub.execute_input":"2023-12-06T21:06:11.445222Z","iopub.status.idle":"2023-12-06T21:06:11.450659Z","shell.execute_reply.started":"2023-12-06T21:06:11.445196Z","shell.execute_reply":"2023-12-06T21:06:11.449690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Status\n- image caption generation pipeline is working\n- add gui for dragging in pictures \n- train for meme generation on images","metadata":{}},{"cell_type":"code","source":"# # working with flicker 8k dataset \n# #location of the training data \n# data_location =  \"../input/flickr8k\"\n# # #copy dataloader\n# # !cp ../input/data-loader/data_loader.py .\n\n# #imports\n# import numpy as np\n# import torch\n# from torch.utils.data import DataLoader,Dataset\n# import torchvision.transforms as T\n\n# #custom imports \n# from data_loader import FlickrDataset,get_data_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:11.452752Z","iopub.execute_input":"2023-12-06T21:06:11.453192Z","iopub.status.idle":"2023-12-06T21:06:11.462566Z","shell.execute_reply.started":"2023-12-06T21:06:11.453160Z","shell.execute_reply":"2023-12-06T21:06:11.461806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References: \n- https://www.kaggle.com/code/mdteach/image-captioning-with-attention-pytorch\n- https://www.kaggle.com/code/mdteach/image-captioning-with-attention-pytorch","metadata":{}},{"cell_type":"code","source":"data_location =  \"../input/flickr8k\"\n!ls $data_location","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:11.463623Z","iopub.execute_input":"2023-12-06T21:06:11.463917Z","iopub.status.idle":"2023-12-06T21:06:12.400787Z","shell.execute_reply.started":"2023-12-06T21:06:11.463894Z","shell.execute_reply":"2023-12-06T21:06:12.399696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the text data \nimport pandas as pd\ncaption_file = data_location + '/captions.txt'\ndf = pd.read_csv(caption_file)\nprint(\"There are {} image to captions\".format(len(df)))\ndf.head(7)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:12.402566Z","iopub.execute_input":"2023-12-06T21:06:12.402977Z","iopub.status.idle":"2023-12-06T21:06:12.471432Z","shell.execute_reply.started":"2023-12-06T21:06:12.402939Z","shell.execute_reply":"2023-12-06T21:06:12.470589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n#select any index from the whole dataset \n#single image has 5 captions\n#so, select indx as: 1,6,11,16...\ndata_idx = 11\n\n#eg path to be plot: ../input/flickr8k/Images/1000268201_693b08cb0e.jpg\nimage_path = data_location+\"/Images/\"+df.iloc[data_idx,0]\nimg=mpimg.imread(image_path)\nplt.imshow(img)\nplt.show()\n\n#image consits of 5 captions,\n#showing all 5 captions of the image of the given idx \nfor i in range(data_idx,data_idx+5):\n    print(\"Caption:\",df.iloc[i,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:12.474951Z","iopub.execute_input":"2023-12-06T21:06:12.475221Z","iopub.status.idle":"2023-12-06T21:06:12.908683Z","shell.execute_reply.started":"2023-12-06T21:06:12.475197Z","shell.execute_reply":"2023-12-06T21:06:12.907781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports \nimport numpy\nimport os\nfrom collections import Counter\nimport spacy\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:12.909862Z","iopub.execute_input":"2023-12-06T21:06:12.910165Z","iopub.status.idle":"2023-12-06T21:06:16.239329Z","shell.execute_reply.started":"2023-12-06T21:06:12.910139Z","shell.execute_reply":"2023-12-06T21:06:16.238366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U pip setuptools wheel\n\n!pip install -U spacy\n\n!python -m spacy download en_core_web_sm","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:06:16.240528Z","iopub.execute_input":"2023-12-06T21:06:16.241066Z","iopub.status.idle":"2023-12-06T21:07:09.613387Z","shell.execute_reply.started":"2023-12-06T21:06:16.241033Z","shell.execute_reply":"2023-12-06T21:07:09.612391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using spacy for the better text tokenization \nspacy_eng = spacy.load(\"en_core_web_sm\")\n\n#example\ntext = \"This is a good place to find a city\"\n[token.text.lower() for token in spacy_eng.tokenizer(text)]","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:09.614943Z","iopub.execute_input":"2023-12-06T21:07:09.615263Z","iopub.status.idle":"2023-12-06T21:07:10.754368Z","shell.execute_reply.started":"2023-12-06T21:07:09.615233Z","shell.execute_reply":"2023-12-06T21:07:10.753384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        #setting the pre-reserved tokens int to string tokens\n        self.itos = {0:\"<PAD>\",1:\"<START>\",2:\"<END>\",3:\"<UNKNOWN>\"}\n        \n        #string to int tokens\n        #its reverse dict self.itos\n        self.stoi = {v:k for k,v in self.itos.items()}\n        \n        self.freq_threshold = freq_threshold\n        \n    def __len__(self): return len(self.itos)\n    \n    @staticmethod\n    def tokenize(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def build_vocab(self, sentence_list):\n        frequencies = Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                frequencies[word] += 1\n                \n                #add the word to the vocab if it reaches minum frequecy threshold\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self,text):\n        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n        tokenized_text = self.tokenize(text)\n        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNKNOWN>\"] for token in tokenized_text ]    ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.755520Z","iopub.execute_input":"2023-12-06T21:07:10.755849Z","iopub.status.idle":"2023-12-06T21:07:10.765693Z","shell.execute_reply.started":"2023-12-06T21:07:10.755823Z","shell.execute_reply":"2023-12-06T21:07:10.764705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the vicab class \nv = Vocabulary(freq_threshold=1)\n\nv.build_vocab([\"This is a good place to find a city\"])\nprint(v.stoi)\nprint(v.numericalize(\"This is a good place to find a city here!!\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.767006Z","iopub.execute_input":"2023-12-06T21:07:10.767622Z","iopub.status.idle":"2023-12-06T21:07:10.781542Z","shell.execute_reply.started":"2023-12-06T21:07:10.767588Z","shell.execute_reply":"2023-12-06T21:07:10.780698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    \"\"\"\n    FlickrDataset\n    \"\"\"\n    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_file)\n        self.transform = transform\n        \n        #Get image and caption colum from the dataframe\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        #Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.captions.tolist())\n        \n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        caption = self.captions[idx]\n        img_name = self.imgs[idx]\n        img_location = os.path.join(self.root_dir,img_name)\n        img = Image.open(img_location).convert(\"RGB\")\n        \n        #apply the transfromation to the image\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        #numericalize the caption text\n        caption_vec = []\n        caption_vec += [self.vocab.stoi[\"<START>\"]]\n        caption_vec += self.vocab.numericalize(caption)\n        caption_vec += [self.vocab.stoi[\"<END>\"]]\n        \n        return img, torch.tensor(caption_vec)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.782869Z","iopub.execute_input":"2023-12-06T21:07:10.783108Z","iopub.status.idle":"2023-12-06T21:07:10.792401Z","shell.execute_reply.started":"2023-12-06T21:07:10.783086Z","shell.execute_reply":"2023-12-06T21:07:10.791685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defing the transform to be applied\ntransforms = T.Compose([\n    T.Resize((224,224)),\n    T.ToTensor()\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.793547Z","iopub.execute_input":"2023-12-06T21:07:10.793965Z","iopub.status.idle":"2023-12-06T21:07:10.805500Z","shell.execute_reply.started":"2023-12-06T21:07:10.793941Z","shell.execute_reply":"2023-12-06T21:07:10.804693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.807004Z","iopub.execute_input":"2023-12-06T21:07:10.807335Z","iopub.status.idle":"2023-12-06T21:07:10.815634Z","shell.execute_reply.started":"2023-12-06T21:07:10.807305Z","shell.execute_reply":"2023-12-06T21:07:10.814921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    captions_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n\n\n\nimg, caps = dataset[0]\nshow_image(img,\"Image\")\nprint(\"Token:\",caps)\nprint(\"Sentence:\")\nprint([dataset.vocab.itos[token] for token in caps.tolist()])","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:10.819682Z","iopub.execute_input":"2023-12-06T21:07:10.819989Z","iopub.status.idle":"2023-12-06T21:07:13.563169Z","shell.execute_reply.started":"2023-12-06T21:07:10.819959Z","shell.execute_reply":"2023-12-06T21:07:13.562313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CapsCollate:\n    \"\"\"\n    Collate to apply the padding to the captions with dataloader\n    \"\"\"\n    def __init__(self,pad_idx,batch_first=False):\n        self.pad_idx = pad_idx\n        self.batch_first = batch_first\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs,dim=0)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n        return imgs,targets","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:13.564520Z","iopub.execute_input":"2023-12-06T21:07:13.564918Z","iopub.status.idle":"2023-12-06T21:07:13.571482Z","shell.execute_reply.started":"2023-12-06T21:07:13.564884Z","shell.execute_reply":"2023-12-06T21:07:13.570663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#writing the dataloader\n#setting the constants\nBATCH_SIZE = 4\nNUM_WORKER = 1\n\n#token to represent the padding\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:13.572639Z","iopub.execute_input":"2023-12-06T21:07:13.572944Z","iopub.status.idle":"2023-12-06T21:07:13.618644Z","shell.execute_reply.started":"2023-12-06T21:07:13.572920Z","shell.execute_reply":"2023-12-06T21:07:13.617814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generating the iterator from the dataloader\ndataiter = iter(data_loader)\n\n#getting the next batch\nbatch = next(dataiter)\n\n#unpacking the batch\nimages, captions = batch\n\n#showing info of image in single batch\nfor i in range(BATCH_SIZE):\n    img,cap = images[i],captions[i]\n    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n    eos_index = caption_label.index('<END>')\n    caption_label = caption_label[1:eos_index]\n    caption_label = ' '.join(caption_label)                      \n    show_image(img,caption_label)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:13.620029Z","iopub.execute_input":"2023-12-06T21:07:13.620342Z","iopub.status.idle":"2023-12-06T21:07:15.851816Z","shell.execute_reply.started":"2023-12-06T21:07:13.620318Z","shell.execute_reply":"2023-12-06T21:07:15.850706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiate the Dataset and Dataloader\n\n#setting the constants\ndata_location =  \"../input/flickr8k\"\nBATCH_SIZE = 256\n# BATCH_SIZE = 6\nNUM_WORKER = 4\n\n#defining the transform to be applied\ntransforms = T.Compose([\n    T.Resize(226),                     \n    T.RandomCrop(224),                 \n    T.ToTensor(),                               \n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    captions_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n\n#writing the dataloader\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n\n    # batch_first=False\n)\n\n#vocab_size\nvocab_size = len(dataset.vocab)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:15.853383Z","iopub.execute_input":"2023-12-06T21:07:15.853718Z","iopub.status.idle":"2023-12-06T21:07:17.542319Z","shell.execute_reply.started":"2023-12-06T21:07:15.853691Z","shell.execute_reply":"2023-12-06T21:07:17.541404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.543496Z","iopub.execute_input":"2023-12-06T21:07:17.543801Z","iopub.status.idle":"2023-12-06T21:07:17.548979Z","shell.execute_reply.started":"2023-12-06T21:07:17.543776Z","shell.execute_reply":"2023-12-06T21:07:17.548013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n\n    def forward(self, images):\n        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n        return features","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.550117Z","iopub.execute_input":"2023-12-06T21:07:17.550438Z","iopub.status.idle":"2023-12-06T21:07:17.560766Z","shell.execute_reply.started":"2023-12-06T21:07:17.550408Z","shell.execute_reply":"2023-12-06T21:07:17.559950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bahdanau Attention\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n        super(Attention, self).__init__()\n        \n        self.attention_dim = attention_dim\n        \n        self.W = nn.Linear(decoder_dim,attention_dim)\n        self.U = nn.Linear(encoder_dim,attention_dim)\n        \n        self.A = nn.Linear(attention_dim,1)\n        \n        \n        \n        \n    def forward(self, features, hidden_state):\n        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n        \n        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n        \n        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n        \n        \n        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n        \n        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n        \n        return alpha,attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.561971Z","iopub.execute_input":"2023-12-06T21:07:17.562294Z","iopub.status.idle":"2023-12-06T21:07:17.570866Z","shell.execute_reply.started":"2023-12-06T21:07:17.562265Z","shell.execute_reply":"2023-12-06T21:07:17.570057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Attention Decoder\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        \n        #save the model param\n        self.vocab_size = vocab_size\n        self.attention_dim = attention_dim\n        self.decoder_dim = decoder_dim\n        \n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        \n        \n        self.fcn = nn.Linear(decoder_dim,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n        \n        \n    \n    def forward(self, features, captions):\n        \n        #vectorize the caption\n        embeds = self.embedding(captions)\n        \n        # Initialize LSTM state\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        #get the seq length to iterate\n        seq_length = len(captions[0])-1 #Exclude the last one\n        batch_size = captions.size(0)\n        num_features = features.size(1)\n        \n        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n                \n        for s in range(seq_length):\n            alpha,context = self.attention(features, h)\n            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n                    \n            output = self.fcn(self.drop(h))\n            \n            preds[:,s] = output\n            alphas[:,s] = alpha  \n        \n        \n        return preds, alphas\n    \n    def generate_caption(self,features,max_len=20,vocab=None):\n        # Inference part\n        # Given the image features generate the captions\n        \n        batch_size = features.size(0)\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        alphas = []\n        \n        #starting input\n        word = torch.tensor(vocab.stoi['<START>']).view(1,-1).to(device)\n        embeds = self.embedding(word)\n\n        \n        captions = []\n        \n        for i in range(max_len):\n            alpha,context = self.attention(features, h)\n            \n            \n            #store the apla score\n            alphas.append(alpha.cpu().detach().numpy())\n            \n            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            output = output.view(batch_size,-1)\n        \n            \n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n            \n            #save the generated word\n            captions.append(predicted_word_idx.item())\n            \n            #end if <EOS detected>\n            if vocab.itos[predicted_word_idx.item()] == \"<END>\":\n                break\n            \n            #send generated word as the next caption\n            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        #covert the vocab idx to words and return sentence\n        return [vocab.itos[idx] for idx in captions],alphas\n    \n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.572160Z","iopub.execute_input":"2023-12-06T21:07:17.572412Z","iopub.status.idle":"2023-12-06T21:07:17.590369Z","shell.execute_reply.started":"2023-12-06T21:07:17.572390Z","shell.execute_reply":"2023-12-06T21:07:17.589516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderRNN(\n            embed_size=embed_size,\n            vocab_size = len(dataset.vocab),\n            attention_dim=attention_dim,\n            encoder_dim=encoder_dim,\n            decoder_dim=decoder_dim\n        )\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.591406Z","iopub.execute_input":"2023-12-06T21:07:17.591687Z","iopub.status.idle":"2023-12-06T21:07:17.603768Z","shell.execute_reply.started":"2023-12-06T21:07:17.591642Z","shell.execute_reply":"2023-12-06T21:07:17.602944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparams\nembed_size=300\nvocab_size = len(dataset.vocab)\nattention_dim=256\nencoder_dim=2048\ndecoder_dim=512\nlearning_rate = 3e-4","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.604807Z","iopub.execute_input":"2023-12-06T21:07:17.605085Z","iopub.status.idle":"2023-12-06T21:07:17.616497Z","shell.execute_reply.started":"2023-12-06T21:07:17.605062Z","shell.execute_reply":"2023-12-06T21:07:17.615692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#init model\nmodel = EncoderDecoder(\n    embed_size=300,\n    vocab_size = len(dataset.vocab),\n    attention_dim=256,\n    encoder_dim=2048,\n    decoder_dim=512\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:17.617568Z","iopub.execute_input":"2023-12-06T21:07:17.618882Z","iopub.status.idle":"2023-12-06T21:07:21.785659Z","shell.execute_reply.started":"2023-12-06T21:07:17.618857Z","shell.execute_reply":"2023-12-06T21:07:21.784629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#helper function to save the model\ndef save_model(model,num_epochs):\n    model_state = {\n        'num_epochs':num_epochs,\n        'embed_size':embed_size,\n        'vocab_size':len(dataset.vocab),\n        'attention_dim':attention_dim,\n        'encoder_dim':encoder_dim,\n        'decoder_dim':decoder_dim,\n        'state_dict':model.state_dict()\n    }\n\n    torch.save(model_state,'attention_model_state.pth')","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:21.786871Z","iopub.execute_input":"2023-12-06T21:07:21.787157Z","iopub.status.idle":"2023-12-06T21:07:21.792739Z","shell.execute_reply.started":"2023-12-06T21:07:21.787132Z","shell.execute_reply":"2023-12-06T21:07:21.791702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nprint_every = 100\n\nfor epoch in range(1,num_epochs+1):   \n    for idx, (image, captions) in enumerate(iter(data_loader)):\n        image,captions = image.to(device),captions.to(device)\n\n        # Zero the gradients.\n        optimizer.zero_grad()\n\n        # Feed forward\n        outputs,attentions = model(image, captions)\n\n        # Calculate the batch loss.\n        targets = captions[:,1:]\n        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n        \n        # Backward pass.\n        loss.backward()\n\n        # Update the parameters in the optimizer.\n        optimizer.step()\n\n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n            \n            \n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)\n                \n            model.train()\n        \n    #save the latest model\n    save_model(model,epoch)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T21:07:21.793833Z","iopub.execute_input":"2023-12-06T21:07:21.794170Z","iopub.status.idle":"2023-12-06T21:31:16.451977Z","shell.execute_reply.started":"2023-12-06T21:07:21.794147Z","shell.execute_reply":"2023-12-06T21:31:16.450736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next steps we have this pretrained model: train it with the instagram captions of a celeberty to fine tune. ---> Become more personalized. ","metadata":{},"execution_count":null,"outputs":[]}]}